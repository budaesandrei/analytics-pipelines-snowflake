{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Pipeline DevOps - Firebase to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Firebase Poject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To setup a firebase project navigate to this [link](https://console.firebase.google.com/) and click on **Create a project** and follow the setup on the screen. Make sure to **Enable Google Analytics** for the project\n",
    "2. Navigate to the **Project Settings** > **Integration** > **Big Query** > **Manage** and make sure that **Google Analytics** from the **Exported Integrations** is **On** and in the **Export Settings** tick at least **Daily** (*Note: You will need to switch to the Blaze Plan to enable Streaming*)\n",
    "3. After a few days of app runtime, the event data should have collected in Big Query. Navigate to [Google Cloud Console](https://console.cloud.google.com/) and enable **Billing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to authenticate but in this template we will create and use a **Service Account**:\n",
    "1. Navigate to [IAM & Roles](https://console.cloud.google.com/iam-admin/serviceaccounts) and select your project\n",
    "2. Click **+ CREATE SERVICE ACCOUNT** and add a service account name (e.g. bigquery-python), add a **description** (optional) and click **CREATE AND CONTINUE**\n",
    "3. Add **Roles** from the dropdown and **+ ADD ANOTHER ROLE** as many times as needed then **CONTINUE**:\n",
    "    * Add **Basic** > **Owner** role\n",
    "    * Add **BigQuery Data Viewer** role\n",
    "4. Click **Done**\n",
    "5. Once the **Service Account** has been created, click on the three dots under **Action** and click **Manage Keys**\n",
    "6. From the next page, click on **ADD KEY** > **Create New Key** > **JSON** > **Create**\n",
    "7. This will download a one time file on your computer. **DO NOT LOSE THIS FILE** as it cannot be downloaded again. If you do lose it, you can delete the current key and create a new one (from Step 5)\n",
    "8. Rename the file to **\"gccreds.json\"** and drop it in the same folder as this notebook (if you want to keep it in more secure location I suggest you save the path in an environment variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an account in Snowflake (e.g. data_loader) with a role that has **ACCOUNTADMIN** rights (e.g. data_loader and assign the accountadmin role to this role)\n",
    "2. (For Windows) In the Search Bar search for **Environment Variables** and open **Edit the system environment variables** \n",
    "3. Click on **Environment Variables** and in the **User variables** section click **New**\n",
    "4. In the **Variable name** type in **ETL_SN_CREDS**\n",
    "5. In the **Variable value** insert a **JSON** in the format\n",
    "```JSON\n",
    "{\"username\":\"data_loader\", \"password\":\"<PASSWORD>\",\"account\":\"<ACCOUNT>\",\"warehouse\":\"<WAREHOUSE>\",\"role\":\"data_loader\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Google Cloud APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access services programatically, Google requires you to manually enable some APIs:\n",
    "1. Enable the [Access Management (IAM) API](https://console.cloud.google.com/flows/enableapi?apiid=iam.googleapis.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a python terminal (I use **Anaconda Prompt**) and run\n",
    "```python\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import pubsub_v1\n",
    "from google.cloud import bigquery_datatransfer\n",
    "from googleapiclient import discovery\n",
    "\n",
    "from google.api_core import exceptions\n",
    "from googleapiclient import errors\n",
    "\n",
    "import snowflake.connector as sf\n",
    "\n",
    "import json, os, datetime, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gccreds = json.loads(open('gccreds.json', 'rt').read()) # Or change the path to the location you have saved the service account details\n",
    "sncreds = json.loads(os.environ['ETL_SN_CREDS'])\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_info(gccreds)\n",
    "\n",
    "storage_client = storage.Client(credentials=credentials)\n",
    "bq_client = bigquery.Client(credentials=credentials)\n",
    "publisher = pubsub_v1.PublisherClient(credentials=credentials)\n",
    "subscriber = pubsub_v1.SubscriberClient(credentials=credentials)\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient(credentials=credentials)\n",
    "\n",
    "iam_service = discovery.build(\n",
    "    serviceName='iam',\n",
    "    version='v1',\n",
    "    credentials=credentials)\n",
    "\n",
    "crm_service = discovery.build(\n",
    "    serviceName='cloudresourcemanager',\n",
    "    version='v1',\n",
    "    credentials=credentials)\n",
    "\n",
    "conn = sf.connect(\n",
    "    user=sncreds['username'],\n",
    "    password=sncreds['password'],\n",
    "    account=sncreds['account'],\n",
    "    warehouse=sncreds['warehouse'],\n",
    "    role=sncreds['role']\n",
    ")\n",
    "\n",
    "def run(sql):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'prod' # Use 'prod' or 'dev' only\n",
    "app_name = input('Please insert the app name: ')\n",
    "database = input('Please insert the Snowflake database name where you want data to stream to: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = gccreds['project_id']\n",
    "dataset_id = [x.dataset_id for x in bq_client.list_datasets() if 'analytics_' in x.dataset_id][0]\n",
    "location = bq_client.get_dataset(dataset_id).location\n",
    "\n",
    "datasource = \"bigquery\"\n",
    "gc_format = re.sub(r'[^0-9a-zA-Z]+', '-', app_name).lower()\n",
    "sn_format = re.sub(r'[^0-9a-zA-Z]+', '_', app_name).lower()\n",
    "dssn_format = sn_format + \"_\" + datasource\n",
    "\n",
    "bucket_name = f\"{gc_format}-event-data\"\n",
    "topic_name = f\"{bucket_name}-file-created\"\n",
    "subscription_name = topic_name\n",
    "storage_service_agent = storage_client.get_service_account_email()\n",
    "\n",
    "storage_integration = f\"{dssn_format}_storage_integration_{env.lower()}\"\n",
    "notification_integration = f\"{dssn_format}_notification_integration_{env.lower()}\"\n",
    "\n",
    "schema = sn_format\n",
    "stage = f\"{dssn_format}_stage\"\n",
    "pipe = f\"{dssn_format}_pipe\"\n",
    "table = \"raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an IAM Role that Snowflake can use to Access Google Cloud Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = iam_service.projects().roles().create(\n",
    "        parent='projects/' + project_id,\n",
    "        body={\n",
    "            'roleId':'snowflake',\n",
    "            'role':{\n",
    "                'title':'Snowflake Integration',\n",
    "                'description':f'Created on: {datetime.datetime.utcnow().date():%Y-%m-%d}\\nThis role is used by Snowflake to communicate with Google Cloud Storage',\n",
    "                'includedPermissions':['storage.buckets.get', 'storage.objects.get', 'storage.objects.list'],\n",
    "                'stage':'ALPHA'\n",
    "            }\n",
    "        }).execute()\n",
    "except errors.HttpError as e:\n",
    "    if 'already exists' in e.reason: print(f\"Custom IAM Role 'snowflake' already exists\")\n",
    "    role = {\n",
    "        'name':f'projects/{project_id}/roles/snowflake'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Google Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = 'STANDARD'\n",
    "    new_bucket = storage_client.create_bucket(\n",
    "        bucket_or_name=bucket,\n",
    "        location=location)\n",
    "\n",
    "    print(f\"Created bucket {new_bucket.name} in {new_bucket.location} with storage class {new_bucket.storage_class}\")\n",
    "except exceptions.Conflict as e:\n",
    "    if 'You already own this bucket' in e.message: print(f\"Bucket '{bucket_name}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake Storage Integration with Google Cloud Storage Bucket and Get the Service Account Created by Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "create storage integration if not exists {storage_integration}\n",
    "  type = external_stage\n",
    "  storage_provider = gcs\n",
    "  enabled = true\n",
    "  storage_allowed_locations = ('gcs://{bucket_name}/')\n",
    "'''\n",
    "run(sql)\n",
    "print(f\"Storage Integration {storage_integration} has been created if it didn't already exist\")\n",
    "\n",
    "sql = f'''\n",
    "desc storage integration {storage_integration}\n",
    "'''\n",
    "cursor = conn.cursor(sf.DictCursor)\n",
    "cursor.execute(sql)\n",
    "storage_gcp_service_account = [x['property_value'] for x in cursor if x['property'] == 'STORAGE_GCP_SERVICE_ACCOUNT'][0]\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Snowflake Permissions to the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_policy = bucket.get_iam_policy()\n",
    "_ = [x for x in bucket_policy.bindings if role['name'] in x['role']] == []\n",
    "if _:\n",
    "    bucket_policy.bindings.append({\n",
    "        \"role\": role[\"name\"],\n",
    "        \"members\": {\n",
    "            f\"serviceAccount:{storage_gcp_service_account}\"\n",
    "        }\n",
    "    })\n",
    "    bucket.set_iam_policy(bucket_policy)\n",
    "    print(f\"Added '{storage_gcp_service_account}' with role '{role['name']}' to bucket '{bucket.name}'\")\n",
    "else:\n",
    "    print(f\"'{storage_gcp_service_account}' with role '{role['name']}' to bucket '{bucket.name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pub/Sub Topic and Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_path = publisher.topic_path(project_id, topic_name)\n",
    "subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "try:\n",
    "    topic = publisher.create_topic(\n",
    "        request={\n",
    "            \"name\": topic_path\n",
    "        }\n",
    "    )\n",
    "except exceptions.AlreadyExists as e:\n",
    "    if topic_name in e.message: print(f\"Topic {topic_name} already exists\")\n",
    "        \n",
    "try:\n",
    "    subscription = subscriber.create_subscription(request={\"name\": subscription_path, \"topic\": topic_path})\n",
    "    print(f\"Created Subscription '{subscription.name}'\")\n",
    "except exceptions.AlreadyExists as e:\n",
    "    if subscription_name in e.message: print(f\"Subscription {subscription_name} already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bucket Permissions to the Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_policy = publisher.get_iam_policy(request={\n",
    "    \"resource\":topic_path\n",
    "})\n",
    "_ = [x for x in topic_policy.bindings if f\"serviceAccount:{storage_service_agent}\" in x.members] == []\n",
    "if _:\n",
    "    topic_policy.bindings.add(\n",
    "        role=\"roles/pubsub.publisher\",\n",
    "        members=[\n",
    "            f\"serviceAccount:{storage_service_agent}\"\n",
    "        ])\n",
    "    publisher.set_iam_policy(request={\n",
    "        \"resource\":topic_path,\n",
    "        \"policy\":topic_policy\n",
    "    })\n",
    "    print(f\"Added '{storage_service_agent}' with role 'roles/pubsub.publisher' to topic '{topic_name}'\")\n",
    "else:\n",
    "    print(f\"'{storage_service_agent}' with role 'roles/pubsub.publisher' to topic '{topic_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Notification from Bucket to Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [x for x in bucket.list_notifications() if topic_name == x.topic_name] == []\n",
    "if _:\n",
    "    notification = bucket.notification(\n",
    "        topic_name=topic_name,\n",
    "        payload_format='JSON_API_V1',\n",
    "        event_types='OBJECT_FINALIZE'\n",
    "    ).create()\n",
    "    print(f\"Created Notification to topic '{topic_name}' from bucket '{bucket_name}'\")\n",
    "else:\n",
    "    print(f\"Notification to topic '{topic_name}' from bucket '{bucket_name}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake Notification Integration with Google Cloud Subscription and Get the Service Account Created by Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "create notification integration if not exists {notification_integration}\n",
    "  type = queue\n",
    "  notification_provider = gcp_pubsub\n",
    "  enabled = true\n",
    "  gcp_pubsub_subscription_name = 'projects/{project_id}/subscriptions/{subscription_name}'\n",
    "'''\n",
    "run(sql)\n",
    "print(f\"Notification Integration {notification_integration} has been created if it didn't already exist\")\n",
    "\n",
    "sql = f'''\n",
    "desc notification integration {notification_integration}\n",
    "'''\n",
    "cursor = conn.cursor(sf.DictCursor)\n",
    "cursor.execute(sql)\n",
    "gcp_pubsub_service_account = [x['property_value'] for x in cursor if x['property'] == 'GCP_PUBSUB_SERVICE_ACCOUNT'][0]\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Snowflake Permissions to the Subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_policy = subscriber.get_iam_policy(request={\n",
    "    \"resource\":f\"projects/{project_id}/subscriptions/{subscription_name}\"\n",
    "})\n",
    "_ = [x for x in subscription_policy.bindings if f\"serviceAccount:{gcp_pubsub_service_account}\" in x.members] == []\n",
    "if _:\n",
    "    subscription_policy.bindings.add(\n",
    "        role=\"roles/pubsub.subscriber\",\n",
    "        members=[\n",
    "            f\"serviceAccount:{gcp_pubsub_service_account}\"\n",
    "        ])\n",
    "    subscriber.set_iam_policy(request={\n",
    "        \"resource\":f\"projects/{project_id}/subscriptions/{subscription_name}\",\n",
    "        \"policy\":subscription_policy\n",
    "    })\n",
    "    print(f\"Added '{gcp_pubsub_service_account}' with role 'roles/pubsub.subscriber' to subscription '{subscription_name}'\")\n",
    "else:\n",
    "    print(f\"'{gcp_pubsub_service_account}' with role 'roles/pubsub.subscriber' to subscription '{subscription_name}' already exists.\")\n",
    "subscriber.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add *Monitoring Viewer* role to the Snowflake Pub/Sub Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitoring_policy = (\n",
    "    crm_service.projects()\n",
    "    .getIamPolicy(\n",
    "        resource=project_id\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "_ = [x for x in monitoring_policy[\"bindings\"] if f\"serviceAccount:{gcp_pubsub_service_account}\" in x[\"members\"]] == []\n",
    "if _:\n",
    "    binding = {\n",
    "        \"role\": \"roles/monitoring.viewer\",\n",
    "        \"members\": [f\"serviceAccount:{gcp_pubsub_service_account}\"]\n",
    "    }\n",
    "    monitoring_policy[\"bindings\"].append(binding)\n",
    "    crm_service.projects().setIamPolicy(\n",
    "        resource=project_id,\n",
    "        body={\"policy\":monitoring_policy}\n",
    "    ).execute()\n",
    "    print(f\"Added '{gcp_pubsub_service_account}' with role 'roles/monitoring.viewer' to project\")\n",
    "else:\n",
    "    print(f\"'{gcp_pubsub_service_account}' with role 'roles/monitoring.viewer' to project already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Snowflake Database, Schema, Variant Table and File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''create database if not exists {database}'''\n",
    "run(sql)\n",
    "print(f\"Database {database} has been created if it didn't already exist\")\n",
    "\n",
    "sql = f'''use database {database}'''\n",
    "run(sql)\n",
    "\n",
    "sql = f'''create schema if not exists {schema}\n",
    "comment = 'This schema has been created to store the raw data generated by the {app_name} app and subsequent aggregations and views' '''\n",
    "run(sql)\n",
    "print(f\"Schema {schema} has been created if it didn't already exist\")\n",
    "\n",
    "sql = f'''use schema {schema}'''\n",
    "run(sql)\n",
    "\n",
    "sql = f'''create table if not exists {table} (\n",
    "    filename varchar(1024),\n",
    "    file_row_number int,\n",
    "    json_data variant,\n",
    "    Primary Key(json_data)\n",
    ")\n",
    "comment = 'This JSON Table contains the data sent by the {app_name} app' '''\n",
    "run(sql)\n",
    "print(f\"Table {table} has been created if it didn't already exist\")\n",
    "\n",
    "sql = f'''create file format if not exists json\n",
    "type = 'json'\n",
    "compression = 'gzip'\n",
    "strip_outer_array = true\n",
    "comment = 'This file format has been created to be used by snowpipe when copying the BigQuery generated data into the {table} table' '''\n",
    "run(sql)\n",
    "print(f\"File Format JSON has been created if it didn't already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Snowflake Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "create stage if not exists {stage}\n",
    "  url='gcs://{bucket_name}/'\n",
    "  storage_integration = {storage_integration};\n",
    "'''\n",
    "run(sql)\n",
    "print(f\"Stage {stage} has been created if it didn't already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Snowflake Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "create pipe if not exists {pipe}\n",
    "  auto_ingest = true\n",
    "  integration = {notification_integration}\n",
    "  comment = 'This pipe is automatically injesting data from the {stage} stage into the raw table once a notification of file creation has been transmitted from Google Pub/Sub'\n",
    "  as\n",
    "copy into {table} from\n",
    "(select\n",
    "    metadata$filename,\n",
    "    metadata$file_row_number,\n",
    "    t.$1\n",
    "from\n",
    "    @{stage}\n",
    "    (file_format => json) t)\n",
    "'''\n",
    "run(sql)\n",
    "print(f\"Pipe {pipe} has been created if it didn't already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BigQuery Scheduled Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "DECLARE tables ARRAY <STRING>;\n",
    "\n",
    "/************************************************************************************************************\n",
    "*                                             DAILY EVENTS EXPORT                                           *\n",
    "************************************************************************************************************/\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS `{project_id}.{dataset_id}.daily_export_log`\n",
    "(\n",
    "    table_name STRING,\n",
    "    insert_date TIMESTAMP\n",
    ")\n",
    "OPTIONS\n",
    "(\n",
    "  expiration_timestamp=TIMESTAMP \"3000-01-01\"\n",
    ");\n",
    "\n",
    "SET tables = (SELECT\n",
    "    ARRAY_AGG(TABLE_NAME) TABLES\n",
    "FROM\n",
    "    `{project_id}.{dataset_id}.INFORMATION_SCHEMA.TABLES`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(TABLE_NAME, 'events_\\\\\\\\d{{8}}') AND\n",
    "    TABLE_NAME NOT IN (SELECT TABLE_NAME FROM `{project_id}.{dataset_id}.daily_export_log`)\n",
    ");\n",
    "\n",
    "\n",
    "FOR tab IN \n",
    "    (SELECT * FROM UNNEST(tables))\n",
    "DO\n",
    "    EXECUTE IMMEDIATE '''\n",
    "    EXPORT DATA\n",
    "    OPTIONS ( \n",
    "        uri = CONCAT('gs://{bucket_name}/live/', format_timestamp('%Y/%m/%d/', current_timestamp()), ''' || \"'\" || tab.f0_ || \"'\" || ''', '/*_', format_timestamp('%Y%m%d%H%M%S', current_timestamp()), '.json.gz'),\n",
    "        format='JSON',\n",
    "        compression='GZIP',\n",
    "        overwrite=FALSE \n",
    "        ) AS\n",
    "    SELECT * FROM `{project_id}.{dataset_id}.''' || tab.f0_ || '''` \n",
    "    ''';\n",
    "\n",
    "    EXECUTE IMMEDIATE '''\n",
    "    INSERT INTO `{project_id}.{dataset_id}.daily_export_log` SELECT ''' || \"'\" || tab.f0_ || \"'\" || ''' table_name, current_timestamp() insert_date\n",
    "    ''';\n",
    "\n",
    "    EXECUTE IMMEDIATE '''\n",
    "    ALTER TABLE `{project_id}.{dataset_id}.''' || tab.f0_ || '''`\n",
    "    ADD COLUMN IF NOT EXISTS gcs_export_timestamp TIMESTAMP\n",
    "    ''';\n",
    "\n",
    "    EXECUTE IMMEDIATE '''\n",
    "    UPDATE `{project_id}.{dataset_id}.''' || tab.f0_ || '''` SET\n",
    "    gcs_export_timestamp = current_timestamp()\n",
    "    WHERE gcs_export_timestamp IS NULL\n",
    "    ''';\n",
    "END FOR;\n",
    "\n",
    "\n",
    "/************************************************************************************************************\n",
    "*                                             DELAYED EVENTS EXPORT                                         *\n",
    "************************************************************************************************************/\n",
    "\n",
    "SET tables = (SELECT\n",
    "  ARRAY_AGG(DISTINCT destination_table.table_id) TABLES\n",
    "FROM\n",
    "  `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT\n",
    "WHERE\n",
    "  job_type = 'LOAD' and\n",
    "  REGEXP_CONTAINS(destination_table.table_id, 'events_\\\\d{8}')\n",
    "  AND creation_time >= DATE_ADD(CURRENT_TIMESTAMP(), INTERVAL -1 HOUR)\n",
    ");\n",
    "\n",
    "FOR tab IN \n",
    "    (SELECT * FROM UNNEST(tables))\n",
    "DO\n",
    "/************  CHECK IF DAILY TABLE WAS OVERWRITTEN AND RE-ADD THE gcs_export_timestamp COLUMN  ************/\n",
    "    BEGIN\n",
    "        IF (SELECT 1 FROM `region-us`.INFORMATION_SCHEMA.COLUMNS WHERE table_name = tab.f0_ AND column_name = 'gcs_export_timestamp') IS NULL THEN\n",
    "            IF (SELECT 1 FROM `{project_id}.{dataset_id}.daily_export_log` WHERE table_name = tab.f0_) IS NOT NULL THEN\n",
    "                EXECUTE IMMEDIATE '''\n",
    "                ALTER TABLE `{project_id}.{dataset_id}.''' || tab.f0_ || '''`\n",
    "                ADD COLUMN IF NOT EXISTS gcs_export_timestamp TIMESTAMP\n",
    "                ''';\n",
    "\n",
    "                EXECUTE IMMEDIATE '''\n",
    "                UPDATE `{project_id}.{dataset_id}.''' || tab.f0_ || '''` SET\n",
    "                gcs_export_timestamp = (SELECT insert_date FROM `{project_id}.{dataset_id}.daily_export_log` WHERE table_name = \\'''' || tab.f0_ || '''\\')\n",
    "                WHERE gcs_export_timestamp IS NULL\n",
    "                ''';\n",
    "            END IF;\n",
    "        END IF;\n",
    "    END;\n",
    "/*****************************  EXPORT DATA WHERE gcs_export_timestamp IS NULL  ****************************/\n",
    "    BEGIN\n",
    "        EXECUTE IMMEDIATE '''\n",
    "        CREATE OR REPLACE TEMP TABLE _SESSION.tmp AS\n",
    "        SELECT * FROM `{project_id}.{dataset_id}.''' || tab.f0_ || '''` WHERE gcs_export_timestamp IS NULL\n",
    "        ''';\n",
    "\n",
    "        IF (SELECT COUNT(*) cnt FROM _SESSION.tmp) > 0 THEN\n",
    "            EXECUTE IMMEDIATE '''\n",
    "            EXPORT DATA\n",
    "            OPTIONS ( \n",
    "                uri = CONCAT('gs://{bucket_name}/delayed/', format_timestamp('%Y/%m/%d/', current_timestamp()), ''' || \"'\" || tab.f0_ || \"'\" || ''', '/*_', format_timestamp('%Y%m%d%H%M%S', current_timestamp()), '.json.gz'),\n",
    "                format='JSON',\n",
    "                compression='GZIP',\n",
    "                overwrite=FALSE \n",
    "                ) AS\n",
    "            SELECT * FROM _SESSION.tmp \n",
    "            ''';\n",
    "\n",
    "            EXECUTE IMMEDIATE '''\n",
    "            UPDATE `{project_id}.{dataset_id}.''' || tab.f0_ || '''` SET\n",
    "            gcs_export_timestamp = current_timestamp()\n",
    "            WHERE gcs_export_timestamp IS NULL\n",
    "            ''';\n",
    "        END IF;\n",
    "    EXCEPTION WHEN ERROR THEN END;\n",
    "END FOR;\n",
    "\"\"\"\n",
    "\n",
    "transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "    display_name=\"BigQuery to GCS Daily Backup\",\n",
    "    data_source_id=\"scheduled_query\",\n",
    "    params={\n",
    "        \"query\": sql\n",
    "    },\n",
    "    schedule=\"every 1 hours\",\n",
    ")\n",
    "\n",
    "transfer_config = transfer_client.create_transfer_config(\n",
    "    bigquery_datatransfer.CreateTransferConfigRequest(\n",
    "        parent=f\"projects/{project_id}/locations/{location.lower()}\",\n",
    "        transfer_config=transfer_config,\n",
    "        service_account_name=gccreds['client_email'],\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Created scheduled query '{}'\".format(transfer_config.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
